## ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ 

1. äº†è§£æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ
2. æŒæ¡æ¨¡å‹è®­ç»ƒ/å¾®è°ƒ/å°å‚æ•°é‡å¾®è°ƒçš„æ“ä½œè¿‡ç¨‹
3. æŒæ¡æ¨¡å‹å¾®è°ƒ/å°å‚æ•°é‡å¾®è°ƒå…³é”®ã€Œè¶…å‚ã€
4. æŒæ¡è®­ç»ƒæ•°æ®çš„é€‰æ‹©ã€å‡†å¤‡ã€æ¸…æ´—ç­‰æ–¹æ³•ä¸æ€è·¯
5. è®­ç»ƒä¸€ä¸ªå‚ç›´é¢†åŸŸçš„å¤§æ¨¡å‹

å¼€å§‹ä¸Šè¯¾ï¼


## ğŸ“ è¿™å ‚è¯¾æ€ä¹ˆå­¦

ä»£ç èƒ½åŠ›è¦æ±‚ï¼š**ä¸­**ï¼ŒAI/æ•°å­¦åŸºç¡€è¦æ±‚ï¼š**ä¸­é«˜**

1. **è¿™ä¸¤å ‚è¯¾å†…å®¹æœ‰éš¾åº¦ï¼Œæ˜¯æ•´é—¨è¯¾é‡Œæœ€éš¾çš„ä¸¤å ‚ï¼ˆæ²¡æœ‰ä¹‹ä¸€ï¼‰**
   1. æœ‰å¾ˆå¤šé™Œç”Ÿçš„åè¯ï¼ŒåŒ…æ‹¬æ•°å­¦åè¯å’Œæ¨¡å‹ç®—æ³•æœ¬èº«çš„åè¯
   2. æ¶‰åŠåˆ°å¾ˆå¤šæ•°å­¦çŸ¥è¯†ï¼Œå¾ˆå¤šä¸œè¥¿æœ¬èº«æ˜¯ä»æ•°å­¦æ¨å¯¼å‡ºæ¥çš„ï¼Œä¸å¥½å…·è±¡åŒ–
   3. æ·±åº¦å­¦ä¹ é‡Œæœ‰å¤§é‡åŸºäºç»éªŒçš„æ€»ç»“ï¼Œä½“ç°æˆå„ç§è¶…å‚å’Œ Tricks
2. **è¿™å ‚è¯¾è¯¥æ€ä¹ˆå­¦**
   1. ä¹‹å‰æ¥è§¦è¿‡æœºå™¨å­¦ä¹ çš„åŒå­¦
      - å°½é‡å¤šç†è§£ç†è®ºéƒ¨åˆ†ï¼Œè¡¥å……ä¹‹å‰ç¼ºå¤±çš„çŸ¥è¯†
      - æ¯å¤šç†è§£ä¸€ä¸ªçŸ¥è¯†ç‚¹ï¼Œå°±æ¯”åˆ«äººæ›´èµ„æ·±äº†ä¸€ç‚¹
   2. ç¼–ç¨‹åŸºç¡€æ‰å®ï¼Œä½†å®Œå…¨æ²¡æ¥è§¦è¿‡æœºå™¨å­¦ä¹ çš„åŒå­¦
      - å…ˆå­¦ä¼šæ•´ä¸ªæµç¨‹ï¼Œèƒ½ä¸Šæ‰‹è·‘å®éªŒæ˜¯è‡³å…³é‡è¦çš„ç¬¬ä¸€æ­¥
      - å°½é‡ç†è§£ä¸€äº›åŸºæœ¬åŸç†ï¼Œè‡³å°‘æœªæ¥æƒ³è¿›ä¸€æ­¥äº†è§£åŸç†æ—¶çŸ¥é“é‚£ä¸ªé—®é¢˜å’Œä»€ä¹ˆæ¦‚å¿µæœ‰å…³
   3. ç¼–ç¨‹ä»ä¸ç†Ÿç»ƒçš„åŒå­¦
      - å…ˆå½“ç§‘æ™®ææ–™å¬ï¼Œæ„Ÿå—ä¸€ä¸‹ AI æ¨¡å‹çš„åº•å±‚æ˜¯æ€ä¹ˆå·¥ä½œçš„
      - æŠ“ç´§è¡¥å……åŸºç¡€èƒ½åŠ›ï¼Œä¸ç§¯è·¬æ­¥ï¼Œæ— ä»¥è‡³åƒé‡Œ
3. æ·±åº¦å­¦ä¹ æ˜¯åŸºäºæ•°å­¦çš„ç»éªŒç§‘å­¦
   1. ç¨‹åºå‘˜æ€ç»´ï¼šIF...ELSE... _â€œä½ å°±å‘Šè¯‰æˆ‘é‡åˆ°è¿™ä¸ªæƒ…å†µæ€ä¹ˆåŠâ€_
   2. ç®—æ³•å·¥ç¨‹å¸ˆæ€ç»´ï¼š$P(åŠæ³•|æƒ…å†µ)$ _â€œè¿™ä¸ªæƒ…å†µ**å¤§æ¦‚ç‡**ä½ å¯ä»¥**å°è¯•**XXXâ€_
4. æ·±åº¦å­¦ä¹ è¿™ä¸ªé¢†åŸŸå°±åƒæ”€ç å³°
   1. ä¸å­˜åœ¨æ·å¾„ï¼Œè‡³å°‘ç›®å‰è¿˜æ²¡æœ‰
   2. æ ¹æ®è‡ªèº«æ¡ä»¶ï¼Œè§åˆ°é£æ™¯ã€æ”€ä¸Šä¸€åº§å°å³°ã€è¶…è¶Šè‡ªå·±åŸæ¥çš„é«˜åº¦ï¼Œéƒ½æ˜¯æ”¶è·ï¼åŠ æ²¹ï¼


<div class="alert alert-info">
<b>é¢å‘åˆå­¦è€…çš„æ·±åº¦å­¦ä¹ è¯¾ï¼š</b> 
<ol>
<li>å´æ©è¾¾ã€Šäººäºº AIã€‹(ç‰¹åˆ«é€šä¿—) https://www.zhihu.com/education/video-course/1556316449043668992</li>
<li>ææ²çš„æ·±åº¦å­¦ä¹ è¯¾ (ç¨å¾®æ·±ä¸€ç‚¹) https://www.zhihu.com/education/video-course/1647604835598092705</li>
</ol>
åœ¨è¿™ä¸ªæ›´å¹¿æ³›çš„å®šä½ä¸Šï¼Œå·²ç»æœ‰å¾ˆå¤šä¼˜ç§€çš„è¯¾ç¨‹ã€‚æœ¬è¯¾ç¨‹åªé’ˆå¯¹å¤§æ¨¡å‹å¾®è°ƒçš„çŸ¥è¯†åŸºç¡€å±•å¼€ã€‚
</div>


## ä»€ä¹ˆæ—¶å€™éœ€è¦ Fine-Tuning

1. æœ‰ç§æœ‰éƒ¨ç½²çš„éœ€æ±‚
2. å¼€æºæ¨¡å‹åŸç”Ÿçš„èƒ½åŠ›ä¸æ»¡è¶³ä¸šåŠ¡éœ€æ±‚

## å…ˆçœ‹ä¸€ä¸ªä¾‹å­


http://localhost:6006/

**è®¢é…’åº—æœºå™¨äºº**

```json
[
  {
    "role": "user",
    "content": "æ‚¨å¥½ï¼Œæˆ‘è¦æ‰¾ä¸€å®¶èˆ’é€‚å‹é…’åº—ä½å®¿ï¼Œç„¶åå¸Œæœ›é…’åº—æä¾›æš–æ°”ä¸è¡Œæå¯„å­˜ã€‚"
  },
  {
    "role": "search",
    "arguments": {
      "facilities": ["æš–æ°”", "è¡Œæå¯„å­˜"],
      "type": "èˆ’é€‚å‹"
    }
  },
  {
    "role": "return",
    "records": [
      {
        "name": "åŒ—äº¬é¦™æ±Ÿæˆ´æ–¯é…’åº—",
        "type": "èˆ’é€‚å‹",
        "address": "åŒ—äº¬ä¸œåŸåŒºå—æ²³æ²¿å¤§è¡—å—æ¹¾å­èƒ¡åŒ1å·",
        "subway": "å¤©å®‰é—¨ä¸œåœ°é“ç«™Bå£",
        "phone": "010-65127788",
        "facilities": [
          "é…’åº—å„å¤„æä¾›wifi",
          "å›½é™…é•¿é€”ç”µè¯",
          "å¹é£æœº",
          "24å°æ—¶çƒ­æ°´",
          "æš–æ°”",
          "è¥¿å¼é¤å…",
          "ä¸­å¼é¤å…",
          "æ®‹ç–¾äººè®¾æ–½",
          "ä¼šè®®å®¤",
          "æ— çƒŸæˆ¿",
          "å•†åŠ¡ä¸­å¿ƒ",
          "æ—©é¤æœåŠ¡",
          "æ¥æœºæœåŠ¡",
          "æ¥å¾…å¤–å®¾",
          "æ´—è¡£æœåŠ¡",
          "è¡Œæå¯„å­˜",
          "ç§Ÿè½¦",
          "å«é†’æœåŠ¡"
        ],
        "price": 328.0,
        "rating": 4.2,
        "hotel_id": 10
      }
    ]
  },
  {
    "role": "assistant",
    "content": "é‚£æ¨èæ‚¨åŒ—äº¬é¦™æ±Ÿæˆ´æ–¯é…’åº—ï¼Œç¬¦åˆæ‚¨çš„å…¨éƒ¨ä½å®¿è¦æ±‚å¹¶ä¸”ä»·æ ¼åˆæ¯”è¾ƒåˆé€‚çš„é…’åº—ã€‚"
  },
  {
    "role": "user",
    "content": "è¿™å®¶é…’åº—çš„ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ"
  },
  {
    "role": "assistant",
    "content": "è¿™å®¶é…’åº—çš„ä»·æ ¼æ˜¯æ¯æ™š328.0å…ƒã€‚"
  },
  {
    "role": "user",
    "content": "å¥½çš„ï¼Œé‚£å°±é¢„è®¢åŒ—äº¬é¦™æ±Ÿæˆ´æ–¯é…’åº—å§ï¼"
  },
  {
    "role": "assistant",
    "content": "å¥½çš„ï¼Œç¥æ‚¨å…¥ä½æ„‰å¿«ï¼"
  }
]
```


## ä¸€ã€å…ˆæ‰¾æ‰¾æ„Ÿè§‰


ä¸Šæ‰‹æ“ä½œä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š

- **æƒ…æ„Ÿåˆ†ç±»**
  - è¾“å…¥ï¼šç”µå½±è¯„è®º
  - è¾“å‡ºï¼šæ ‡ç­¾ \['neg','pos'\]
  - æ•°æ®æºï¼šhttps://huggingface.co/datasets/rotten_tomatoes


### 1.1ã€**å·¥å…·**ï¼šä»‹ç»ä¸€ä¸ªæ¨¡å‹è®­ç»ƒåˆ©å™¨ Hugging Face

- å®˜ç½‘ï¼šhttp://www.huggingface.co
- ç›¸å½“äºé¢å‘ NLP æ¨¡å‹çš„ Github
- å°¤å…¶åŸºäº transformer çš„å¼€æºæ¨¡å‹éå¸¸å…¨
- å°è£…äº†æ¨¡å‹ã€æ•°æ®é›†ã€è®­ç»ƒå™¨ç­‰ï¼Œä½¿æ¨¡å‹çš„ä¸‹è½½ã€ä½¿ç”¨ã€è®­ç»ƒéƒ½éå¸¸æ–¹ä¾¿

**å®‰è£…ä¾èµ–**


```python
# pipå®‰è£…
pip install transformers # å®‰è£…æœ€æ–°çš„ç‰ˆæœ¬
pip install transformers == 4.30 # å®‰è£…æŒ‡å®šç‰ˆæœ¬
# condaå®‰è£…
conda install -c huggingface transformers  # åª4.0ä»¥åçš„ç‰ˆæœ¬
```


### 1.2ã€æ“ä½œæµç¨‹


<br />
<img src="training_process.png" style="margin-left: 0px" width="600px">
<br />

<div class="alert alert-warning">
<b>æ³¨æ„ï¼š</b> 
<ul>
<li>ä»¥ä¸‹çš„ä»£ç ï¼Œéƒ½ä¸è¦åœ¨Jupyterç¬”è®°ä¸Šç›´æ¥è¿è¡Œï¼Œä¼šæ­»æœºï¼ï¼</li>
<li>è¯·ä¸‹è½½å·¦è¾¹çš„è„šæœ¬`experiments/tiny/train.py`ï¼Œåœ¨å®éªŒæœåŠ¡å™¨ä¸Šè¿è¡Œã€‚</li>
</ul>
</div>


1. å¯¼å…¥ç›¸å…³åº“


```python
import datasets
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModel
from transformers import AutoModelForCausalLM
from transformers import TrainingArguments, Seq2SeqTrainingArguments
from transformers import Trainer, Seq2SeqTrainer
import transformers
from transformers import DataCollatorWithPadding
from transformers import TextGenerationPipeline
import torch
import numpy as np
import os, re
from tqdm import tqdm
import torch.nn as nn
```


2. åŠ è½½**æ•°æ®é›†**

é€šè¿‡ HuggingFaceï¼Œå¯ä»¥æŒ‡å®šæ•°æ®é›†åç§°ï¼Œè¿è¡Œæ—¶è‡ªåŠ¨ä¸‹è½½


```python
# æ•°æ®é›†åç§°
DATASET_NAME = "rotten_tomatoes"

# åŠ è½½æ•°æ®é›†
raw_datasets = load_dataset(DATASET_NAME)

# è®­ç»ƒé›†
raw_train_dataset = raw_datasets["train"]

# éªŒè¯é›†
raw_valid_dataset = raw_datasets["validation"]
```


3. åŠ è½½**æ¨¡å‹**

é€šè¿‡ HuggingFaceï¼Œå¯ä»¥æŒ‡å®šæ¨¡å‹åç§°ï¼Œè¿è¡Œæ—¶è‡ªåŠ¨ä¸‹è½½


```python
# æ¨¡å‹åç§°
MODEL_NAME = "gpt2"

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,trust_remote_code=True)
```


4. åŠ è½½ **Tokenizer**

é€šè¿‡ HuggingFaceï¼Œå¯ä»¥æŒ‡å®šæ¨¡å‹åç§°ï¼Œè¿è¡Œæ—¶è‡ªåŠ¨ä¸‹è½½å¯¹åº” Tokenizer


```python
# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,trust_remote_code=True)
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
tokenizer.pad_token_id = 0
```


```python
# å…¶å®ƒç›¸å…³å…¬å…±å˜é‡èµ‹å€¼

# è®¾ç½®éšæœºç§å­ï¼šåŒä¸ªç§å­çš„éšæœºåºåˆ—å¯å¤ç°
transformers.set_seed(42)

# æ ‡ç­¾é›†
named_labels = ['neg','pos']

# æ ‡ç­¾è½¬ token_id
label_ids = [
    tokenizer(named_labels[i],add_special_tokens=False)["input_ids"][0]
    for i in range(len(named_labels))
]
```


5. **å¤„ç†æ•°æ®é›†**ï¼šè½¬æˆæ¨¡å‹æ¥å—çš„è¾“å…¥æ ¼å¼
   - æ‹¼æ¥è¾“å…¥è¾“å‡ºï¼š\<INPUT TOKEN IDS\>\<EOS_TOKEN_ID\>\<OUTPUT TOKEN IDS\>
   - PAD æˆç›¸ç­‰é•¿åº¦ï¼š
     - <INPUT 1.1><INPUT 1.2>...\<EOS_TOKEN_ID\>\<OUTPUT TOKEN IDS\>\<PAD\>...\<PAD\>
     - <INPUT 2.1><INPUT 2.2>...\<EOS_TOKEN_ID\>\<OUTPUT TOKEN IDS\>\<PAD\>...\<PAD\>
   - æ ‡è¯†å‡ºå‚ä¸ Loss è®¡ç®—çš„ Tokens (åªæœ‰è¾“å‡º Token å‚ä¸ Loss è®¡ç®—)
     - \<-100\>\<-100\>...\<OUTPUT TOKEN IDS\>\<-100\>...\<-100\>


```python
MAX_LEN=32   #æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆè¾“å…¥+è¾“å‡ºï¼‰
DATA_BODY_KEY = "text" # æ•°æ®é›†ä¸­çš„è¾“å…¥å­—æ®µå
DATA_LABEL_KEY = "label" #æ•°æ®é›†ä¸­è¾“å‡ºå­—æ®µå

# å®šä¹‰æ•°æ®å¤„ç†å‡½æ•°ï¼ŒæŠŠåŸå§‹æ•°æ®è½¬æˆinput_ids, attention_mask, labels
def process_fn(examples):
    model_inputs = {
            "input_ids": [],
            "attention_mask": [],
            "labels": [],
        }
    for i in range(len(examples[DATA_BODY_KEY])):
        inputs = tokenizer(examples[DATA_BODY_KEY][i],add_special_tokens=False)
        label = label_ids[examples[DATA_LABEL_KEY][i]]
        input_ids = inputs["input_ids"] + [tokenizer.eos_token_id, label]

        raw_len = len(input_ids)
        input_len = len(inputs["input_ids"]) + 1

        if raw_len >= MAX_LEN:
            input_ids = input_ids[-MAX_LEN:]
            attention_mask = [1] * MAX_LEN
            labels = [-100]*(MAX_LEN - 1) + [label]
        else:
            input_ids = input_ids + [tokenizer.pad_token_id] * (MAX_LEN - raw_len)
            attention_mask = [1] * raw_len + [0] * (MAX_LEN - raw_len)
            labels = [-100]*input_len + [label] + [-100] * (MAX_LEN - raw_len)
        model_inputs["input_ids"].append(input_ids)
        model_inputs["attention_mask"].append(attention_mask)
        model_inputs["labels"].append(labels)
    return model_inputs
```


```python
# å¤„ç†è®­ç»ƒæ•°æ®é›†
tokenized_train_dataset = raw_train_dataset.map(
    process_fn,
    batched=True,
    remove_columns=raw_train_dataset.columns,
    desc="Running tokenizer on train dataset",
)

# å¤„ç†éªŒè¯æ•°æ®é›†
tokenized_valid_dataset = raw_valid_dataset.map(
    process_fn,
    batched=True,
    remove_columns=raw_valid_dataset.columns,
    desc="Running tokenizer on validation dataset",
)
```


6. å®šä¹‰**æ•°æ®è§„æ•´å™¨**ï¼šè®­ç»ƒæ—¶è‡ªåŠ¨å°†æ•°æ®æ‹†åˆ†æˆ **Batch**


```python
# å®šä¹‰æ•°æ®æ ¡å‡†å™¨ï¼ˆè‡ªåŠ¨ç”Ÿæˆbatchï¼‰
collater = DataCollatorWithPadding(
    tokenizer=tokenizer, return_tensors="pt",
)
```


7. å®šä¹‰è®­ç»ƒ **è¶…å‚**ï¼šæ¯”å¦‚**å­¦ä¹ ç‡**


```python
LR=2e-5         # å­¦ä¹ ç‡
BATCH_SIZE=8    # Batchå¤§å°
INTERVAL=100    # æ¯å¤šå°‘æ­¥æ‰“ä¸€æ¬¡ log / åšä¸€æ¬¡ eval

# å®šä¹‰è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./output",              # checkpointä¿å­˜è·¯å¾„
    evaluation_strategy="steps",        # æŒ‰æ­¥æ•°è®¡ç®—evalé¢‘ç‡
    overwrite_output_dir=True,
    num_train_epochs=1,                 # è®­ç»ƒepochæ•°
    per_device_train_batch_size=BATCH_SIZE,     # æ¯å¼ å¡çš„batchå¤§å°
    gradient_accumulation_steps=1,              # ç´¯åŠ å‡ ä¸ªstepåšä¸€æ¬¡å‚æ•°æ›´æ–°
    per_device_eval_batch_size=BATCH_SIZE,      # evaluation batch size
    eval_steps=INTERVAL,                # æ¯Næ­¥evalä¸€æ¬¡
    logging_steps=INTERVAL,             # æ¯Næ­¥logä¸€æ¬¡
    save_steps=INTERVAL,                # æ¯Næ­¥ä¿å­˜ä¸€ä¸ªcheckpoint
    learning_rate=LR,                   # å­¦ä¹ ç‡
)
```


8. å®šä¹‰**è®­ç»ƒå™¨**


```python
# èŠ‚çœæ˜¾å­˜
model.gradient_checkpointing_enable()

# å®šä¹‰è®­ç»ƒå™¨
trainer = Trainer(
    model=model, # å¾…è®­ç»ƒæ¨¡å‹
    args=training_args, # è®­ç»ƒå‚æ•°
    data_collator=collater, # æ•°æ®æ ¡å‡†å™¨
    train_dataset=tokenized_train_dataset,  # è®­ç»ƒé›†
    eval_dataset=tokenized_valid_dataset,   # éªŒè¯é›†
    # compute_metrics=compute_metric,         # è®¡ç®—è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
)
```


9. å¼€å§‹è®­ç»ƒ


```python
# å¼€å§‹è®­ç»ƒ
trainer.train()
```


10. åŠ è½½è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼ˆå‚è€ƒï¼‰

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# åŠ è½½è®­ç»ƒåçš„ checkpoint
model = AutoModelForCausalLM.from_pretrained("output/checkpoint-1000")

# æ¨¡å‹è®¾ä¸ºæ¨ç†æ¨¡å¼
model.eval()

# åŠ è½½ tokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# å¾…åˆ†ç±»æ–‡æœ¬
text = "This is a good movie!"

# æ–‡æœ¬è½¬ token ids - è®°å¾—ä»¥ eos æ ‡è¯†è¾“å…¥ç»“æŸï¼Œä¸è®­ç»ƒæ—¶ä¸€æ ·
inputs = tokenizer(text+tokenizer.eos_token, return_tensors="pt")

# æ¨ç†ï¼šé¢„æµ‹æ ‡ç­¾
output = model.generate(**inputs, do_sample=False, max_new_tokens=1)

# label token è½¬æ ‡ç­¾æ–‡æœ¬
tokenizer.decode(output[0][-1])
```

ã€ŒCheckpointã€æŒ‡çš„æ˜¯åœ¨ç‰¹å®šæ—¶é—´ç‚¹ä¿å­˜çš„æ¨¡å‹çš„çŠ¶æ€ã€‚è¿™ä¸ªçŠ¶æ€åŒ…æ‹¬äº†æ¨¡å‹çš„å‚æ•°æƒé‡å’Œä¼˜åŒ–å™¨çš„çŠ¶æ€ï¼Œä½¿å¾—è®­ç»ƒå¯ä»¥ä»è¿™ä¸ªç‚¹é‡æ–°å¼€å§‹è€Œä¸æ˜¯ä»å¤´å¼€å§‹ã€‚

é€šå¸¸ï¼Œæˆ‘ä»¬é€šè¿‡è§‚å¯Ÿåœ¨éªŒè¯é›†ä¸Šçš„è¯„ä¼°ç»“æœï¼Œé€‰æ‹©æŸä¸ª checkpoint ä½œä¸ºæœ€ç»ˆç”¨äºæ¨ç†çš„æ¨¡å‹ã€‚

11. åŠ è½½ checkpoint å¹¶ç»§ç»­è®­ç»ƒï¼ˆé€‰ï¼‰

```python
trainer.train(resume_from_checkpoint="/path/to/checkpoint")
```

<div class="alert alert-warning">
<b>æ³¨æ„ï¼š</b> 
<ul>
<li>ä»¥ä¸Šå®éªŒå»ºè®®åœ¨ 4G ä»¥ä¸Šæ˜¾å­˜çš„ GPU ä¸Šè¿è¡Œ</li>
<li>å¦‚æ˜¾å­˜ä½äº 24G çš„æƒ…å†µä¸‹ï¼Œå¯é€šè¿‡ä¿®æ”¹ä»¥ä¸‹è¶…å‚æ¥é™ä½æ˜¾å­˜éœ€æ±‚</li>
</ul>
</div>

```python
# æ­¤å¤„åªåˆ—å‡ºéœ€ä¿®æ”¹çš„è¶…å‚ï¼Œå…¶å®ƒè¶…å‚ä¸ä¹‹å‰é…ç½®ä¸€è‡´
training_args = TrainingArguments(
    per_device_train_batch_size=1,     # å‡å°æ¯å¼ å¡çš„è®­ç»ƒæ—¶ batch å¤§å°
    gradient_accumulation_steps=8,     # å¢å¤§ç´¯è®¡å‚æ•°æ›´æ–°çš„æ­¥æ•°
    per_device_eval_batch_size=1,      # å‡å°æ¯å¼ å¡çš„è¯„ä¼°æ—¶ batch å¤§å°
)

trainer = Trainer(
    model=model, 
    args=training_args, 
    data_collator=collater, 
    train_dataset=tokenized_train_dataset,  
    eval_dataset=tokenized_valid_dataset,   
    # compute_metrics=compute_metric,         # æ³¨é‡Šæ‰ï¼ä¸ä½¿ç”¨è‡ªå®šä¹‰è¯„ä¼°å™¨
)
```

### æ€»ç»“ä¸Šè¿°è¿‡ç¨‹

1. åŠ è½½æ•°æ®é›†
2. æ•°æ®é¢„å¤„ç†ï¼š
   - å°†è¾“å…¥è¾“å‡ºæŒ‰ç‰¹å®šæ ¼å¼æ‹¼æ¥
   - æ–‡æœ¬è½¬ Token IDs
   - é€šè¿‡ labels æ ‡è¯†å‡ºå“ªéƒ¨åˆ†æ˜¯è¾“å‡ºï¼ˆåªæœ‰è¾“å‡ºçš„ token å‚ä¸ loss è®¡ç®—ï¼‰
3. åŠ è½½æ¨¡å‹ã€Tokenizer
4. å®šä¹‰æ•°æ®è§„æ•´å™¨
5. å®šä¹‰è®­ç»ƒè¶…å‚ï¼šå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ã€...
6. å®šä¹‰è®­ç»ƒå™¨
7. å¼€å§‹è®­ç»ƒ
8. æ³¨æ„ï¼šè®­ç»ƒåæ¨ç†æ—¶ï¼Œè¾“å…¥æ•°æ®çš„æ‹¼æ¥æ–¹å¼è¦ä¸è®­ç»ƒæ—¶ä¸€è‡´

<div class="alert alert-success">
<b>åˆ’é‡ç‚¹ï¼š</b> 
    <ul>
        <li>è®°ä½ä¸Šé¢çš„æµç¨‹ï¼Œä½ å°±èƒ½è·‘é€šæ¨¡å‹è®­ç»ƒè¿‡ç¨‹</li>
        <li>ç†è§£ä¸‹é¢çš„çŸ¥è¯†ï¼Œä½ å°±èƒ½è®­ç»ƒå¥½æ¨¡å‹æ•ˆæœ</li>
    </ul>
</div>


## äºŒã€ä»€ä¹ˆæ˜¯æ¨¡å‹

<div class="alert alert-warning">
<b>å°è¯•ï¼š</b> ç”¨ç®€å•çš„æ•°å­¦è¯­è¨€è¡¨è¾¾æ¦‚å¿µ
</div>

### 2.1ã€é€šä¿—ï¼ˆä¸ä¸¥è°¨ï¼‰çš„è¯´ã€**æ¨¡å‹**æ˜¯ä¸€ä¸ªå‡½æ•°ï¼š

1. å…ˆä¸¾ä¸ªæœ€ç®€å•çš„ä¾‹å­ï¼š
- $y=ax+b$: æè¿°è¾“å…¥ $x$ ä¸è¾“å‡º $y$ çš„å…³ç³»
- è¿™ä¸ªä¾‹å­ä¸­ï¼Œ$x$ ä¸ $y$ æ˜¯ã€Œçº¿æ€§å…³ç³»ã€ï¼Œç”»åœ¨ x-y è½´ä¸Šå°±æ˜¯ä¸€æ¡ç›´çº¿
- å…¶ä¸­ $a$, $b$ æ˜¯å‚æ•°ï¼Œå†³å®šè¿™æ¡ç›´çº¿çš„æ–œç‡å’Œåç¦»åŸç‚¹çš„è·ç¦»
- $a$ å’Œ $b$ æ˜¯æœªçŸ¥çš„ï¼Œéœ€è¦æˆ‘ä»¬ä»ä¸€ç»„æ•°æ®ï¼ˆ$\{(x_i,y_i)\}_{i=1\cdots N}$ï¼‰ä¸­æ¨å¯¼å‡ºæ¥ï¼ˆè®­ç»ƒï¼‰

2. å®é™…é—®é¢˜ä¸­ï¼Œ$x$ ä¸ $y$ ä¸ä¸€å®šæ˜¯ç›´çº¿å…³ç³»ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨ä¸€ä¸ªæ›´å¹¿ä¹‰çš„è¡¨ç¤º

- $y=F(x;\omega)$: $F$ æ˜¯ä»»æ„ä¸€ä¸ªå‡½æ•°å½¢å¼
- å®ƒæ¥æ”¶è¾“å…¥$x$ï¼šå¯ä»¥æ˜¯ä¸€ä¸ªè¯ã€ä¸€ä¸ªå¥å­ã€ä¸€ç¯‡æ–‡ç« æˆ–å›¾ç‰‡ã€è¯­éŸ³ã€è§†é¢‘ ...
  - è¿™äº›ç‰©ä½“éƒ½è¢«è¡¨ç¤ºæˆä¸€ä¸ªæ•°å­¦ã€ŒçŸ©é˜µã€ï¼ˆå…¶å®åº”è¯¥å«å¼ é‡ï¼Œtensorï¼‰
- å®ƒé¢„æµ‹è¾“å‡º$y$
  - å¯ä»¥æ˜¯ã€Œæ˜¯å¦ã€ï¼ˆ{0,1}ï¼‰ã€æ ‡ç­¾ï¼ˆ{0,1,2,3...}ï¼‰ã€ä¸€ä¸ªæ•°å€¼ï¼ˆå›å½’é—®é¢˜ï¼‰ã€ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡ ...
- $F$ çš„æ•°å­¦è¡¨è¾¾å¼å°±æ˜¯ç½‘ç»œç»“æ„ï¼ˆè¿™é‡Œç‰¹æŒ‡æ·±åº¦å­¦ä¹ ï¼‰
- $F$ æœ‰ä¸€ç»„**å‚æ•°** $\omega$ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬è¦è®­ç»ƒçš„éƒ¨åˆ†

<div class="alert alert-warning">
<b>æŠŠå®ƒæƒ³è±¡æˆä¸€ä¸ªæ–¹ç¨‹ï¼š</b> 
    <ol>
        <li>æ¯æ¡æ•°æ®å°±æ˜¯ä¸€å¯¹å„¿ $(x,y)$ ï¼Œå®ƒä»¬æ˜¯å¸¸é‡</li>
        <li>å‚æ•°æ˜¯æœªçŸ¥æ•°ï¼Œæ˜¯å˜é‡</li>
        <li>$F$ å°±æ˜¯è¡¨è¾¾å¼ï¼šæˆ‘ä»¬ä¸çŸ¥é“çœŸå®çš„å…¬å¼æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œæ‰€ä»¥å‡è®¾äº†ä¸€ä¸ªè¶³å¤Ÿå¤æ‚çš„å…¬å¼ï¼ˆæ¯”å¦‚ï¼Œä¸€ä¸ªç‰¹å®šç»“æ„çš„ç¥ç»ç½‘ç»œï¼‰</li>
        <li>è¿™ä¸ªæ±‚è§£è¿™ä¸ªæ–¹ç¨‹ï¼ˆè¿‘ä¼¼è§£ï¼‰å°±æ˜¯è®­ç»ƒè¿‡ç¨‹</li>
    </ol>
</div>

<div class="alert alert-success">
<b>é€šä¿—çš„è®²ï¼š</b> è®­ç»ƒï¼Œå°±æ˜¯ç¡®å®šè¿™ç»„å‚æ•°çš„å–å€¼
    <ul>
        <li>ç”¨æ•°å­¦ï¼ˆæ•°å€¼åˆ†æï¼‰æ–¹æ³•æ‰¾åˆ°ä½¿æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°è¶³å¤Ÿå¥½çš„ä¸€ä¸ªå€¼</li>
        <li>è¡¨ç°è¶³å¤Ÿå¥½ï¼Œå°±æ˜¯è¯´ï¼Œå¯¹æ¯ä¸ªæ•°æ®æ ·æœ¬$(x,y)$ï¼Œä½¿ $F(x;\omega)$ çš„å€¼å°½å¯èƒ½æ¥è¿‘ $y$</li>
    </ul>
</div>

### 2.2ã€ä¸€ä¸ªæœ€ç®€å•çš„ç¥ç»ç½‘ç»œ

ä¸€ä¸ªç¥ç»å…ƒï¼š$y=f(\sum_i w_i\cdot x_i)$

<img src="neuron.jpg" style="margin-left: 0px" width="600px">

æŠŠå¾ˆå¤šç¥ç»å…ƒè¿æ¥èµ·æ¥ï¼Œå°±æˆäº†ç¥ç»ç½‘ç»œï¼š$y=f(\sum_i w_i\cdot x_i)$ã€$z=f(\sum_i w'_i\cdot y_i)$ã€$\tau=f(\sum_i w''_i\cdot z_i)$ã€...

<img src="network.jpg" style="margin-left: 0px" width="600px">

è¿™é‡Œçš„$f$å«æ¿€æ´»å‡½æ•°ï¼Œæœ‰å¾ˆå¤šç§å½¢å¼

ç°ä»Šçš„å¤§æ¨¡å‹ä¸­å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬ï¼šReLUã€GELUã€Swish

<img src="activation.jpeg" style="margin-left: 0px" width="600px">

<div class="alert alert-warning">
<b>æ€è€ƒï¼š</b> è¿™é‡Œå¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ä¼šæ€æ ·ï¼Ÿ
</div>


## ä¸‰ã€ä»€ä¹ˆæ˜¯æ¨¡å‹è®­ç»ƒ

æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä¸€ç»„å‚æ•°$\omega$ï¼Œä½¿æ¨¡å‹é¢„æµ‹çš„è¾“å‡º$\hat{y}=F(x;\omega)$ä¸çœŸå®çš„è¾“å‡º$y$ï¼Œå°½å¯èƒ½çš„æ¥è¿‘

è¿™é‡Œï¼Œæˆ‘ä»¬ï¼ˆè‡³å°‘ï¼‰éœ€è¦ä¸¤ä¸ªè¦ç´ ï¼š

- ä¸€ä¸ªæ•°æ®é›†ï¼ŒåŒ…å«$N$ä¸ªè¾“å…¥è¾“å‡ºçš„ä¾‹å­ï¼ˆç§°ä¸ºæ ·æœ¬ï¼‰ï¼š$D=\{(x_i,y_i)\}_{i=1}^N$
- ä¸€ä¸ª**æŸå¤±å‡½æ•°**ï¼Œè¡¡é‡æ¨¡å‹é¢„æµ‹çš„è¾“å‡ºä¸çœŸå®è¾“å‡ºä¹‹é—´çš„å·®è·ï¼š$\mathrm{loss}(y,F(x;\omega))$
    - ä¾‹å¦‚ï¼š$\mathrm{loss}(y,F(x;\omega))=\|y-F(x;\omega)\|$  

### 3.1ã€æ¨¡å‹è®­ç»ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªæ±‚è§£æœ€ä¼˜åŒ–é—®é¢˜çš„è¿‡ç¨‹

$\min_{\omega} L(D,\omega)$

$L(D,\omega)=\frac{1}{N}\sum_{i=1}^N\mathrm{loss}(y_i,F(x_i;\omega))$

### 3.2ã€æ€ä¹ˆæ±‚è§£

å›å¿†ä¸€ä¸‹æ¢¯åº¦çš„å®šä¹‰

<img src="gradient.svg" style="margin-left: 0px" width="400px">

ä»æœ€ç®€å•çš„æƒ…å†µè¯´èµ·ï¼šæ¢¯åº¦ä¸‹é™ä¸å‡¸é—®é¢˜

<img src="gradient.png" style="margin-left: 0px" width="600px">

æ¢¯åº¦å†³å®šäº†å‡½æ•°å˜åŒ–çš„æ–¹å‘ï¼Œæ¯æ¬¡è¿­ä»£æ›´æ–°æˆ‘ä»¬ä¼šæ”¶æ•›åˆ°ä¸€ä¸ªæå€¼

$\omega_{n+1}\leftarrow \omega_n - \gamma \nabla_{\omega}L(D,\omega)$

å…¶ä¸­ï¼Œ$\gamma<1$å«åš**å­¦ä¹ ç‡**ï¼Œå®ƒå’Œæ¢¯åº¦çš„æ¨¡æ•°å…±åŒå†³å®šäº†æ¯æ­¥èµ°å¤šè¿œ

### 3.3ã€**ç°å®æ€»æ˜¯æ²¡é‚£ä¹ˆç®€å•ï¼ˆ1ï¼‰**ï¼šæ·±åº¦å­¦ä¹ æ²¡æœ‰å…¨å±€æœ€ä¼˜è§£ï¼ˆéå‡¸é—®é¢˜ï¼‰

<br/>
<img src="local_minima.png" style="margin-left: 0px" width="600px">

### 3.4ã€**ç°å®æ€»æ˜¯æ²¡é‚£ä¹ˆç®€å•ï¼ˆ2ï¼‰**ï¼šåœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šæ±‚æ¢¯åº¦ï¼Œè®¡ç®—é‡å¤ªå¤§äº†

<br/>
<img src="batch.png" style="margin-left: 0px" width="600px">

<div class="alert alert-success">
<b>ç»éªŒï¼š</b>
    <ul>
        <li>å¦‚æœå…¨é‡å‚æ•°è®­ç»ƒï¼šæ¡ä»¶å…è®¸çš„æƒ…å†µä¸‹ï¼Œå…ˆå°è¯•Batch Sizeå¤§äº›</li>
        <li>å°å‚æ•°é‡å¾®è°ƒï¼šBatch Size å¤§ä¸ä¸€å®šå°±å¥½ï¼Œçœ‹ç¨³å®šæ€§</li>
    </ul>
</div>

### 3.5ã€**ç°å®æ€»æ˜¯æ²¡é‚£ä¹ˆç®€å•ï¼ˆ3ï¼‰**ï¼šå­¦ä¹ ç‡ä¹Ÿå¾ˆå…³é”®ï¼Œç”šè‡³éœ€è¦åŠ¨æ€è°ƒæ•´

<br/>
<img src="lr.png" style="margin-left: 0px" width="600px">

<div class="alert alert-success">
<b>åˆ’é‡ç‚¹ï¼š</b>é€‚å½“è°ƒæ•´å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰ï¼Œé¿å…é™·å…¥å¾ˆå·®çš„å±€éƒ¨è§£æˆ–è€…è·³è¿‡äº†å¥½çš„è§£
</div>

## å››ã€æ±‚è§£å™¨

ä¸ºäº†è®©è®­ç»ƒè¿‡ç¨‹æ›´å¥½çš„æ”¶æ•›ï¼Œäººä»¬è®¾è®¡äº†å¾ˆå¤šæ›´å¤æ‚çš„æ±‚è§£å™¨

- æ¯”å¦‚ï¼šSGDã€L-BFGSã€Rpropã€RMSpropã€Adamã€AdamWã€AdaGradã€AdaDelta ç­‰ç­‰
- ä½†æ˜¯ï¼Œå¥½åœ¨å¯¹äº Transformer æœ€å¸¸ç”¨çš„å°±æ˜¯ Adam æˆ–è€… AdamW

## äº”ã€ä¸€äº›å¸¸ç”¨çš„**æŸå¤±å‡½æ•°**

- ä¸¤ä¸ªæ•°å€¼çš„å·®è·ï¼ŒMean Squared Errorï¼š$\ell_{\mathrm{MSE}}=\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y}_i)^2$ (ç­‰ä»·äºæ¬§å¼è·ç¦»ï¼Œè§ä¸‹æ–‡)
- ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ï¼ˆæ¬§å¼ï¼‰è·ç¦»ï¼š$\ell(\mathbf{y},\mathbf{\hat{y}})=\|\mathbf{y}-\mathbf{\hat{y}}\|$
- ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„å¤¹è§’ï¼ˆä½™å¼¦è·ç¦»ï¼‰ï¼š
  <img src="cosine_loss.png" style="margin-left: 0px" width="400px">

- ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œäº¤å‰ç†µï¼š$\ell_{\mathrm{CE}}(p,q)=-\sum_i p_i\log q_i$ â€”â€”å‡è®¾æ˜¯æ¦‚ç‡åˆ†å¸ƒ p,q æ˜¯ç¦»æ•£çš„
- è¿™äº›æŸå¤±å‡½æ•°ä¹Ÿå¯ä»¥ç»„åˆä½¿ç”¨ï¼ˆåœ¨æ¨¡å‹è’¸é¦çš„åœºæ™¯å¸¸è§è¿™ç§æƒ…å†µï¼‰ï¼Œä¾‹å¦‚$L=L_1+\lambda L_2$ï¼Œå…¶ä¸­$\lambda$æ˜¯ä¸€ä¸ªé¢„å…ˆå®šä¹‰çš„æƒé‡ï¼Œä¹Ÿå«ä¸€ä¸ªã€Œè¶…å‚ã€

<div class="alert alert-warning">
<b>æ€è€ƒï¼š</b> ä½ èƒ½æ‰¾åˆ°è¿™äº›æŸå¤±å‡½æ•°å’Œåˆ†ç±»ã€èšç±»ã€å›å½’é—®é¢˜ä¹‹é—´çš„å…³ç³»å—ï¼Ÿ
</div>


## å…­ã€å†åŠ¨æ‰‹å¤ä¹ ä¸€ä¸‹ä¸Šè¿°è¿‡ç¨‹

ç”¨ PyTorch è®­ç»ƒä¸€ä¸ªæœ€ç®€å•çš„ç¥ç»ç½‘ç»œ

æ•°æ®é›†ï¼ˆMNISTï¼‰æ ·ä¾‹ï¼š

<img src="MNIST.jpg" style="margin-left: 0px" width="600px">

è¾“å…¥ä¸€å¼  28Ã—28 çš„å›¾åƒï¼Œè¾“å‡ºæ ‡ç­¾ 0--9


<div class="alert alert-warning">
<b>æ³¨æ„ï¼š</b> 
<ul>
<li>ä»¥ä¸‹çš„ä»£ç ï¼Œéƒ½ä¸è¦åœ¨Jupyterç¬”è®°ä¸Šç›´æ¥è¿è¡Œï¼Œä¼šæ­»æœºï¼ï¼</li>
<li>è¯·å°†å·¦ä¾§çš„ `experiments/mnist/train.py` æ–‡ä»¶ä¸‹è½½åˆ°æœ¬åœ°</li>
<li>å®‰è£…ç›¸å…³ä¾èµ–åŒ…: pip install torch torchvision</li>
<li>è¿è¡Œï¼špython3 train.py</li>
<li>æ™®é€šçš„ CPU ä¹Ÿè¶³å¤Ÿè¿è¡Œæ­¤å®éªŒ</li>
</ul>
</div>


```python
from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR

BATCH_SIZE = 64
TEST_BACTH_SIZE = 1000
EPOCHS = 5
LR = 0.01
SEED = 42
LOG_INTERVAL = 100

# å®šä¹‰ä¸€ä¸ªå…¨è¿æ¥ç½‘ç»œ
class FeedForwardNet(nn.Module):
    def __init__(self):
        super().__init__()
        # ç¬¬ä¸€å±‚784ç»´è¾“å…¥ã€256ç»´è¾“å‡º -- å›¾åƒå¤§å°28Ã—28=784
        self.fc1 = nn.Linear(784, 256)
        # ç¬¬äºŒå±‚256ç»´è¾“å…¥ã€128ç»´è¾“å‡º
        self.fc2 = nn.Linear(256, 128)
        # ç¬¬ä¸‰å±‚128ç»´è¾“å…¥ã€64ç»´è¾“å‡º
        self.fc3 = nn.Linear(128, 64)
        # ç¬¬å››å±‚64ç»´è¾“å…¥ã€10ç»´è¾“å‡º -- è¾“å‡ºç±»åˆ«10ç±»ï¼ˆ0,1,...9ï¼‰
        self.fc4 = nn.Linear(64, 10)

        # Dropout module with 0.2 drop probability
        self.dropout = nn.Dropout(p=0.2)

    def forward(self, x):
        # æŠŠè¾“å…¥å±•å¹³æˆ1Då‘é‡
        x = x.view(x.shape[0], -1)

        # æ¯å±‚æ¿€æ´»å‡½æ•°æ˜¯ReLUï¼Œé¢å¤–åŠ dropout
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.dropout(F.relu(self.fc2(x)))
        x = self.dropout(F.relu(self.fc3(x)))

        # è¾“å‡ºä¸º10ç»´æ¦‚ç‡åˆ†å¸ƒ
        x = F.log_softmax(self.fc4(x), dim=1)

        return x

# è®­ç»ƒè¿‡ç¨‹
def train(model, loss_fn, device, train_loader, optimizer, epoch):
    # å¼€å¯æ¢¯åº¦è®¡ç®—
    model.train()
    for batch_idx, (data_input, true_label) in enumerate(train_loader):
        # ä»æ•°æ®åŠ è½½å™¨è¯»å–ä¸€ä¸ªbatch
        # æŠŠæ•°æ®ä¸Šè½½åˆ°GPUï¼ˆå¦‚æœ‰ï¼‰
        data_input, true_label = data_input.to(device), true_label.to(device)
        # æ±‚è§£å™¨åˆå§‹åŒ–ï¼ˆæ¯ä¸ªbatchåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
        optimizer.zero_grad()
        # æ­£å‘ä¼ æ’­ï¼šæ¨¡å‹ç”±è¾“å…¥é¢„æµ‹è¾“å‡º
        output = model(data_input)
        # è®¡ç®—loss
        loss = loss_fn(output, true_label)
        # åå‘ä¼ æ’­ï¼šè®¡ç®—å½“å‰batchçš„lossçš„æ¢¯åº¦
        loss.backward()
        # ç”±æ±‚è§£å™¨æ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°
        optimizer.step()

        # é—´éš”æ€§çš„è¾“å‡ºå½“å‰batchçš„è®­ç»ƒloss
        if batch_idx % LOG_INTERVAL == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data_input), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))


# è®¡ç®—åœ¨æµ‹è¯•é›†çš„å‡†ç¡®ç‡å’Œloss
def test(model, loss_fn, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            # sum up batch loss
            test_loss += loss_fn(output, target, reduction='sum').item()
            # get the index of the max log-probability
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))


def main():
    # æ£€æŸ¥æ˜¯å¦æœ‰GPU
    use_cuda = torch.cuda.is_available()

    # è®¾ç½®éšæœºç§å­ï¼ˆä»¥ä¿è¯ç»“æœå¯å¤ç°ï¼‰
    torch.manual_seed(SEED)

    # è®­ç»ƒè®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
    device = torch.device("cuda" if use_cuda else "cpu")

    # è®¾ç½®batch size
    train_kwargs = {'batch_size': BATCH_SIZE}
    test_kwargs = {'batch_size': TEST_BACTH_SIZE}

    if use_cuda:
        cuda_kwargs = {'num_workers': 1,
                       'pin_memory': True,
                       'shuffle': True}
        train_kwargs.update(cuda_kwargs)
        test_kwargs.update(cuda_kwargs)

    # æ•°æ®é¢„å¤„ç†ï¼ˆè½¬tensorã€æ•°å€¼å½’ä¸€åŒ–ï¼‰
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    # è‡ªåŠ¨ä¸‹è½½MNISTæ•°æ®é›†
    dataset_train = datasets.MNIST('data', train=True, download=True,
                                   transform=transform)
    dataset_test = datasets.MNIST('data', train=False,
                                  transform=transform)

    # å®šä¹‰æ•°æ®åŠ è½½å™¨ï¼ˆè‡ªåŠ¨å¯¹æ•°æ®åŠ è½½ã€å¤šçº¿ç¨‹ã€éšæœºåŒ–ã€åˆ’åˆ†batchã€ç­‰ç­‰ï¼‰
    train_loader = torch.utils.data.DataLoader(dataset_train, **train_kwargs)
    test_loader = torch.utils.data.DataLoader(dataset_test, **test_kwargs)

    # åˆ›å»ºç¥ç»ç½‘ç»œæ¨¡å‹
    model = FeedForwardNet().to(device)

    # æŒ‡å®šæ±‚è§£å™¨
    optimizer = optim.SGD(model.parameters(), lr=LR)
    # scheduler = StepLR(optimizer, step_size=1, gamma=0.9)

    # å®šä¹‰losså‡½æ•°
    # æ³¨ï¼šnll ä½œç”¨äº log_softmax ç­‰ä»·äºäº¤å‰ç†µï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥è‡ªè¡Œæ¨å¯¼
    # https://blog.csdn.net/weixin_38145317/article/details/103288032
    loss_fn = F.nll_loss

    # è®­ç»ƒNä¸ªepoch
    for epoch in range(1, EPOCHS + 1):
        train(model, loss_fn, device, train_loader, optimizer, epoch)
        test(model, loss_fn, device, test_loader)
        # scheduler.step()


if __name__ == '__main__':
    main()
```

    /opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
      from .autonotebook import tqdm as notebook_tqdm


    Train Epoch: 1 [0/60000 (0%)]	Loss: 2.287443
    Train Epoch: 1 [6400/60000 (11%)]	Loss: 2.284967
    Train Epoch: 1 [12800/60000 (21%)]	Loss: 2.273498
    Train Epoch: 1 [19200/60000 (32%)]	Loss: 2.022655
    Train Epoch: 1 [25600/60000 (43%)]	Loss: 1.680803
    Train Epoch: 1 [32000/60000 (53%)]	Loss: 1.322924
    Train Epoch: 1 [38400/60000 (64%)]	Loss: 0.978875
    Train Epoch: 1 [44800/60000 (75%)]	Loss: 0.955985
    Train Epoch: 1 [51200/60000 (85%)]	Loss: 0.670422
    Train Epoch: 1 [57600/60000 (96%)]	Loss: 0.821590
    
    Test set: Average loss: 0.5133, Accuracy: 8522/10000 (85%)
    
    Train Epoch: 2 [0/60000 (0%)]	Loss: 0.740346
    Train Epoch: 2 [6400/60000 (11%)]	Loss: 0.697988
    Train Epoch: 2 [12800/60000 (21%)]	Loss: 0.676830
    Train Epoch: 2 [19200/60000 (32%)]	Loss: 0.531716
    Train Epoch: 2 [25600/60000 (43%)]	Loss: 0.457828
    Train Epoch: 2 [32000/60000 (53%)]	Loss: 0.621303
    Train Epoch: 2 [38400/60000 (64%)]	Loss: 0.354285
    Train Epoch: 2 [44800/60000 (75%)]	Loss: 0.588098
    Train Epoch: 2 [51200/60000 (85%)]	Loss: 0.530143
    Train Epoch: 2 [57600/60000 (96%)]	Loss: 0.533157
    
    Test set: Average loss: 0.3203, Accuracy: 9035/10000 (90%)
    
    Train Epoch: 3 [0/60000 (0%)]	Loss: 0.425095
    Train Epoch: 3 [6400/60000 (11%)]	Loss: 0.301024
    Train Epoch: 3 [12800/60000 (21%)]	Loss: 0.330063
    Train Epoch: 3 [19200/60000 (32%)]	Loss: 0.362905
    Train Epoch: 3 [25600/60000 (43%)]	Loss: 0.387243
    Train Epoch: 3 [32000/60000 (53%)]	Loss: 0.436325
    Train Epoch: 3 [38400/60000 (64%)]	Loss: 0.266472
    Train Epoch: 3 [44800/60000 (75%)]	Loss: 0.463275
    Train Epoch: 3 [51200/60000 (85%)]	Loss: 0.264305
    Train Epoch: 3 [57600/60000 (96%)]	Loss: 0.480805
    
    Test set: Average loss: 0.2456, Accuracy: 9262/10000 (93%)
    
    Train Epoch: 4 [0/60000 (0%)]	Loss: 0.343381
    Train Epoch: 4 [6400/60000 (11%)]	Loss: 0.222288
    Train Epoch: 4 [12800/60000 (21%)]	Loss: 0.200421
    Train Epoch: 4 [19200/60000 (32%)]	Loss: 0.301372
    Train Epoch: 4 [25600/60000 (43%)]	Loss: 0.282800
    Train Epoch: 4 [32000/60000 (53%)]	Loss: 0.424678
    Train Epoch: 4 [38400/60000 (64%)]	Loss: 0.160868
    Train Epoch: 4 [44800/60000 (75%)]	Loss: 0.373828
    Train Epoch: 4 [51200/60000 (85%)]	Loss: 0.273351
    Train Epoch: 4 [57600/60000 (96%)]	Loss: 0.498258
    
    Test set: Average loss: 0.2007, Accuracy: 9388/10000 (94%)
    
    Train Epoch: 5 [0/60000 (0%)]	Loss: 0.175644
    Train Epoch: 5 [6400/60000 (11%)]	Loss: 0.349571
    Train Epoch: 5 [12800/60000 (21%)]	Loss: 0.231020
    Train Epoch: 5 [19200/60000 (32%)]	Loss: 0.277835
    Train Epoch: 5 [25600/60000 (43%)]	Loss: 0.248639
    Train Epoch: 5 [32000/60000 (53%)]	Loss: 0.338623
    Train Epoch: 5 [38400/60000 (64%)]	Loss: 0.174397
    Train Epoch: 5 [44800/60000 (75%)]	Loss: 0.384121
    Train Epoch: 5 [51200/60000 (85%)]	Loss: 0.238978
    Train Epoch: 5 [57600/60000 (96%)]	Loss: 0.279425
    
    Test set: Average loss: 0.1718, Accuracy: 9461/10000 (95%)
    


## [é€‰ä¿®å†…å®¹](optional/index.ipynb)

æ·±åº¦å­¦ä¹ ä¸­è¿˜æœ‰å¾ˆå¤šå¸¸ç”¨æŠ€å·§ï¼Œæ¶‰åŠçš„èƒŒæ™¯çŸ¥è¯†è¾ƒæ·±ï¼Œä¸åœ¨è¯¾ä¸Šå±•å¼€äº†ã€‚

æˆ‘ä»¬é€‰äº†ä¸€äº›æœ€å¸¸ç”¨çš„ï¼Œæ•´ç†æˆäº†éƒ¨åˆ†é€‰ä¿®å†…å®¹ã€‚

å»ºè®®æœ‰æ·±å…¥å­¦ä¹ éœ€æ±‚çš„åŒå­¦è‡ªè¡Œé˜…è¯»ï¼Œæœ‰ç–‘é—®çš„å¯ä»¥åœ¨ç¾¤é‡Œæé—®ã€‚


## ä½œä¸š

åœ¨ HuggingFace ä¸Šæ‰¾ä¸€ä¸ªç®€å•çš„æ•°æ®é›†ï¼Œè‡ªå·±å®ç°ä¸€ä¸ªè®­ç»ƒè¿‡ç¨‹

